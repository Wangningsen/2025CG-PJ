{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAD-Recode Demonstration\n",
    "In this demonstration, we load a point cloud and process it using a pre-trained CAD-Recode model, which generates Python CadQuery code. Finally, we interpret the code to create a CAD model, visualize it, and evaluate its quality metrics. This notebook works on both CPU and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CAD-Recode model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d\n",
    "import trimesh\n",
    "import skimage.io\n",
    "import numpy as np\n",
    "import cadquery as cq\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoTokenizer, Qwen3ForCausalLM, Qwen3Model, PreTrainedModel, AutoModelForCausalLM)\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from pytorch3d.ops import sample_farthest_points\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "class FourierPointEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        frequencies = 2.0 ** torch.arange(8, dtype=torch.float32)\n",
    "        self.register_buffer('frequencies', frequencies, persistent=False)\n",
    "        self.projection = nn.Linear(51, hidden_size)\n",
    "\n",
    "    def forward(self, points):\n",
    "        x = points\n",
    "        x = (x.unsqueeze(-1) * self.frequencies).view(*x.shape[:-1], -1)\n",
    "        x = torch.cat((points, x.sin(), x.cos()), dim=-1)\n",
    "        x = x.to(self.projection.weight.dtype) \n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "        # points shape: (batch_size, num_points, 3)\n",
    "\n",
    "\n",
    "\n",
    "class CADRecode(Qwen3ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        PreTrainedModel.__init__(self, config)\n",
    "        self.model = Qwen3Model(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        \n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        self.point_encoder = FourierPointEncoder(config.hidden_size)\n",
    "        torch.set_default_dtype(torch.bfloat16)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                point_cloud=None,\n",
    "                position_ids=None,\n",
    "                past_key_values=None,\n",
    "                inputs_embeds=None,\n",
    "                labels=None,\n",
    "                use_cache=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None,\n",
    "                cache_position=None):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # concatenate point and text embeddings\n",
    "        if past_key_values is None or past_key_values.get_seq_length() == 0:\n",
    "            assert inputs_embeds is None\n",
    "            inputs_embeds = self.model.embed_tokens(input_ids)\n",
    "            point_embeds = self.point_encoder(point_cloud).bfloat16()\n",
    "            inputs_embeds[attention_mask == -1] = point_embeds.reshape(-1, point_embeds.shape[2])\n",
    "            attention_mask[attention_mask == -1] = 1\n",
    "            input_ids = None\n",
    "            position_ids = None\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position)\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions)\n",
    "\n",
    "    def prepare_inputs_for_generation(self, *args, **kwargs):\n",
    "        model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n",
    "        model_inputs['point_cloud'] = kwargs['point_cloud']\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CAD-Recode checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_PATH = '/home/user2/wns/cad-recode/Qwen3-1.7B'\n",
    "CHECKPOINT_DIR = '/home/user2/wns/cad-recode/checkpoint-65400' \n",
    "POINT_ENCODER_WEIGHTS_PATH = os.path.join(CHECKPOINT_DIR, \"point_encoder_weights.bin\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "attn_implementation = 'flash_attention_2' if torch.cuda.is_available() else None\n",
    "\n",
    "print(f\"Loading full model from checkpoint: {CHECKPOINT_DIR}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    pad_token='<|im_end|>',\n",
    "    padding_side='left',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f\"Tokenizer loaded from: {BASE_MODEL_PATH}\")\n",
    "\n",
    "# 1. 加载原始的基础 LLM 模型 (Qwen3ForCausalLM)\n",
    "print(f\"Loading base LLM (Qwen3ForCausalLM) from: {BASE_MODEL_PATH}\")\n",
    "base_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    attn_implementation=attn_implementation,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"Base LLM loaded.\")\n",
    "\n",
    "# 2. 实例化你的 CADRecode 模型。\n",
    "# 此时，model 的 Qwen 部分和 point_encoder 都是随机初始化的。\n",
    "print(\"Instantiating CADRecode model structure (initially random weights for Qwen and point_encoder)...\")\n",
    "model = CADRecode(base_llm.config) \n",
    "\n",
    "# 3. 将加载了预训练权重的 base_llm 的核心 Qwen 结构赋值给 CADRecode 实例的相应属性。\n",
    "# 这样 CADRecode 的 Qwen 部分就有了预训练权重。\n",
    "model.model = base_llm.model # 赋值 Qwen3Model 实例\n",
    "model.lm_head = base_llm.lm_head # 赋值 Qwen3 的语言模型头\n",
    "print(\"Base LLM's core model and lm_head assigned to CADRecode instance.\")\n",
    "\n",
    "# 4. 定义 PEFT LoRA 配置。\n",
    "# **关键修改：明确指定 target_modules，排除 point_encoder 的线性层。**\n",
    "# Qwen 系列模型的线性层名称通常是：q_proj, k_proj, v_proj, o_proj (注意力层),\n",
    "# 以及 gate_proj, up_proj, down_proj (MLP层)。\n",
    "# 请确保这个列表涵盖了 Qwen3 中所有需要 LoRA 的线性层，但不包含你的 `point_encoder` 中的线性层。\n",
    "peft_config_for_loading = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ], \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 5. **将整个 CADRecode 实例应用 PEFT LoRA。**\n",
    "# 这一步会返回一个 `PeftModelForCausalLM` 实例，它包装了 `CADRecode`。\n",
    "# 由于 `target_modules` 已排除 point_encoder，所以 `point_encoder` 将不会被 LoRA 化。\n",
    "print(\"Applying PEFT LoRA to the CADRecode model structure (only Qwen layers will be wrapped)...\")\n",
    "model = get_peft_model(model, peft_config_for_loading) \n",
    "print(\"CADRecode model (now a PeftModelForCausalLM) prepared.\")\n",
    "\n",
    "# 6. **加载 LoRA 权重。**\n",
    "# `model` 现在是 `PeftModelForCausalLM` 的实例。\n",
    "# `PeftModel` 的 `load_adapter` 方法会从指定目录加载 `adapter_model.safetensors` 并应用到 `model` 中。\n",
    "print(f\"Loading LoRA adapters for base LLM layers from: {CHECKPOINT_DIR}\")\n",
    "model.load_adapter(CHECKPOINT_DIR, adapter_name=\"default\")\n",
    "print(\"LoRA adapters for base LLM layers loaded.\")\n",
    "\n",
    "# 7. **手动加载 point_encoder 的权重。**\n",
    "# `model` 是 `PeftModelForCausalLM`，访问其原始 `CADRecode` 实例需要 `model.base_model.model`。\n",
    "# 由于 `point_encoder` 现在没有被 LoRA 化，`model.base_model.model.point_encoder` 是一个标准的 `FourierPointEncoder`。\n",
    "# 它期望 `point_encoder_state_dict` 包含 `projection.weight` 和 `projection.bias`。\n",
    "print(f\"Attempting to load point_encoder weights from: {POINT_ENCODER_WEIGHTS_PATH}\")\n",
    "if os.path.exists(POINT_ENCODER_WEIGHTS_PATH):\n",
    "    point_encoder_state_dict = torch.load(POINT_ENCODER_WEIGHTS_PATH, map_location='cpu')\n",
    "    \n",
    "    # 这一行应该能成功，因为现在 point_encoder 是非 LoRA 化的，\n",
    "    # 并且你确认保存的权重也是非 LoRA 化的。\n",
    "    model.base_model.model.point_encoder.load_state_dict(point_encoder_state_dict, strict=True) \n",
    "    print(\"Point_encoder weights loaded successfully.\")\n",
    "else:\n",
    "    print(f\"Error: {POINT_ENCODER_WEIGHTS_PATH} not found. 'point_encoder' will be randomly initialized.\")\n",
    "    print(\"Please ensure the specified CHECKPOINT_DIR contains 'point_encoder_weights.bin'.\")\n",
    "\n",
    "# 8. 将整个 CADRecode 模型（现在包含所有组件）移动到设备并设置为评估模式\n",
    "# .to(device) 应该在所有权重加载完成后进行，以确保所有模块都在正确的设备上。\n",
    "model.eval().to(device)\n",
    "print(\"Full model (CADRecode with Qwen3-1.7B + LoRA + trained point_encoder) ready for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load input point cloud\n",
    "Load the mesh of one of the examples from the DeepCAD test set and sample 256 points from it. Then input mesh is normalized to fit within a cube of size 2, centered at the origin. Both the mesh and point cloud can be visualized interactively in this notebook using `open3d` or `trimesh`; however, for non-interactive environments, we render them as static images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/filaPro/cad-recode/releases/download/v1.0/00409751.stl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mesh_to_point_cloud(mesh, n_points=256, n_pre_points=8192):\n",
    "    vertices, _ = trimesh.sample.sample_surface(mesh, n_pre_points)\n",
    "    _, ids = sample_farthest_points(torch.tensor(vertices).unsqueeze(0), K=n_points)\n",
    "    ids = ids[0].numpy()\n",
    "    return np.asarray(vertices[ids])\n",
    "\n",
    "\n",
    "gt_mesh = trimesh.load_mesh('./00409751.stl')\n",
    "gt_mesh.apply_translation(-(gt_mesh.bounds[0] + gt_mesh.bounds[1]) / 2.0)\n",
    "gt_mesh.apply_scale(2.0 / max(gt_mesh.extents))\n",
    "np.random.seed(0)\n",
    "point_cloud = mesh_to_point_cloud(gt_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look at input point cloud\n",
    "It looks like a table with four legs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] Failed to initialize GLEW.\u001b[0;m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'convert_to_pinhole_camera_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m pcd\u001b[38;5;241m.\u001b[39mpaint_uniform_color(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m239\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m     40\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m---> 41\u001b[0m _ \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcd\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(geometry, image_size, camera_distance, image_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m extrinsic[:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mrotation_matrix \u001b[38;5;241m@\u001b[39m eye\n\u001b[1;32m     18\u001b[0m view_control \u001b[38;5;241m=\u001b[39m visualizer\u001b[38;5;241m.\u001b[39mget_view_control()\n\u001b[0;32m---> 19\u001b[0m camera_params \u001b[38;5;241m=\u001b[39m \u001b[43mview_control\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_pinhole_camera_parameters\u001b[49m()\n\u001b[1;32m     20\u001b[0m camera_params\u001b[38;5;241m.\u001b[39mextrinsic \u001b[38;5;241m=\u001b[39m extrinsic\n\u001b[1;32m     21\u001b[0m view_control\u001b[38;5;241m.\u001b[39mconvert_from_pinhole_camera_parameters(camera_params)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'convert_to_pinhole_camera_parameters'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 300x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def render(geometry, image_size=128, camera_distance=-1.8, image_path='/home/user2/wns/cad-recode/tmp/tmp.png'):\n",
    "    visualizer = open3d.visualization.Visualizer()\n",
    "    visualizer.create_window(visible=False)\n",
    "    visualizer.add_geometry(geometry)\n",
    "\n",
    "    lookat = np.array([0, 0, 0], dtype=np.float32)\n",
    "    front = np.array([1, 1, 1], dtype=np.float32)\n",
    "    up = np.array([0, 1, 0], dtype=np.float32)\n",
    "    eye = lookat + front * camera_distance\n",
    "    right = np.cross(up, front)\n",
    "    right /= np.linalg.norm(right)\n",
    "    true_up = np.cross(front, right)\n",
    "    rotation_matrix = np.column_stack((right, true_up, front)).T\n",
    "    extrinsic = np.eye(4)\n",
    "    extrinsic[:3, :3] = rotation_matrix\n",
    "    extrinsic[:3, 3] = -rotation_matrix @ eye\n",
    "\n",
    "    view_control = visualizer.get_view_control()\n",
    "    camera_params = view_control.convert_to_pinhole_camera_parameters()\n",
    "    camera_params.extrinsic = extrinsic\n",
    "    view_control.convert_from_pinhole_camera_parameters(camera_params)\n",
    "\n",
    "    visualizer.poll_events()\n",
    "    visualizer.update_renderer()\n",
    "    visualizer.capture_screen_image(image_path)\n",
    "    visualizer.destroy_window()\n",
    "\n",
    "    image = skimage.io.imread(image_path)\n",
    "    image = skimage.transform.resize(\n",
    "        image,\n",
    "        output_shape=(image_size, image_size),\n",
    "        order=2,\n",
    "        anti_aliasing=True,\n",
    "        preserve_range=True).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "pcd = open3d.geometry.PointCloud()\n",
    "pcd.points = open3d.utility.Vector3dVector(point_cloud)\n",
    "pcd.paint_uniform_color(np.array([0, 80, 239]) / 255)\n",
    "plt.figure(figsize=(3, 3))\n",
    "_ = plt.imshow(render(pcd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CAD-Recode on the input point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a khúc D S S T RZ................................................... ............... ... ... ............ ......... ... ....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ! ... ... ... ... ... ... ... ... ... ... ... ! Name ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ! List 3,3,96,100,3,3,33,97,69,72,72,72,72,98,9\n"
     ]
    }
   ],
   "source": [
    "input_ids = [tokenizer.pad_token_id] * len(point_cloud) + [tokenizer('<|im_start|>')['input_ids'][0]]\n",
    "attention_mask = [-1] * len(point_cloud) + [1]\n",
    "with torch.no_grad():\n",
    "    batch_ids = model.generate(\n",
    "        input_ids=torch.tensor(input_ids).unsqueeze(0).to(model.device),\n",
    "        attention_mask=torch.tensor(attention_mask).unsqueeze(0).to(model.device),\n",
    "        point_cloud=torch.tensor(point_cloud.astype(np.float32)).unsqueeze(0).to(model.device),\n",
    "        max_new_tokens=768,\n",
    "        pad_token_id=tokenizer.pad_token_id)\n",
    "py_string = tokenizer.batch_decode(batch_ids)[0]\n",
    "begin = py_string.find('<|im_start|>') + 12\n",
    "end = py_string.find('<|endoftext|>')\n",
    "py_string = py_string[begin: end]\n",
    "print(py_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute predicted python code to raise a CAD model\n",
    "The predicted code might be invalid or could potentially cause memory leaks in CadQuery, as described in this [issue](https://github.com/CadQuery/cadquery/issues/1665).  Therefore, it is recommended to run the next cell in a separate process with a timeout of, for example, 3 seconds.:\n",
    "```\n",
    "process = Process(target=..., args=...)\n",
    "process.start()\n",
    "process.join(3)\n",
    "\n",
    "if process.is_alive():\n",
    "    process.terminate()\n",
    "    process.join()\n",
    "```\n",
    "But in this demo safety is omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (<string>, line 4)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/envs/cadrecode/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3579\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 1\u001b[0;36m\n\u001b[0;31m    exec(py_string, globals())\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>:4\u001b[0;36m\u001b[0m\n\u001b[0;31m    r=w0.sketch().push([(-5,-93)]).circle(2).reset().face(w0.sketch().segment((69,-96),(75,-96)).segment((75,-93)).segment((69,-93)).arc((68,-95),(69,-96)).assemble()).finalize().extrude(-165).union(w1.sketch().push([(0,-17)]).circle(32).push([(0,-17.5)]).rect(2,1,mode='s').reset().face(w1.sketch().segment((13,-33),(15,-33)).segment((15,-34)).segment((23,-34)).segment((23,-33)).segment((25,-33)).segment((25,-27)).segment((23,-27)).segment((23,-25)).segment((15,-25)).segment((15,-27)).segment((13,-27)).close().assemble(),mode='s').push([(26.5,-34.5)]).rect(1,3,mode='s').reset().face(w1.sketch().segment((45,-33),(49,-33)).segment((49,-27)).segment((45,-27)).close().assemble(),mode='s').reset().face(w1.sketch().segment((45,-15),(49,-15)).segment((49,-10)).segment((45,-10)).close().assemble(),mode='s').finalize().extrude(-17)).union(w0.sketch().segment((-64,-71),(-61,-71)).arc((-55,-77),(-49,-82)).segment((-49,-87)).segment((-43,-87)).arc((-28,-89),(-13,-87)).segment((-9,-87)).segment((-9,-82)).arc((1,-77),(10,-71)).segment((13,-71)).segment((13,-69)).arc((17,-57),(19,-45)).segment((25,-45)).segment((25,-40)).segment((19,-40)).arc((17,-36),(13,-31)).segment((13,-29)).segment((10,-29)).arc((-28,-29),(-43,-40)).segment((-49,-40)).segment((-49,-31)).arc((-55,-36),(-61,-40)).segment((-64,-40)).segment((-64,-42)).arc((-90,-57),(-64,-71)).assemble().reset().face(w0.sketch().segment((-98,71),(-96,71)).arc((-97,75),(-96,80)).segment((-98,80)).close().assemble()).reset().face(w0.sketch().segment((86,-71),(90,-71)).segment((90,-69)).segment((86,-69)).arc((87,-70),(86,-71)).assemble()).finalize().extrude(15)) ) )\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "exec(py_string, globals())\n",
    "compound = globals()['r'].val()\n",
    "vertices, faces = compound.tessellate(0.001, 0.1)\n",
    "mesh = trimesh.Trimesh([(v.x, v.y, v.z) for v in vertices], faces)\n",
    "mesh.export('/home/user2/wns/cad-recode/tmp/333.stl')\n",
    "cq.exporters.export(compound, '/home/user2/wns/cad-recode/tmp/333.step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] Failed to initialize GLEW.\u001b[0;m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'convert_to_pinhole_camera_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m mesh\u001b[38;5;241m.\u001b[39mcompute_vertex_normals()\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m _ \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(geometry, image_size, camera_distance, image_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m extrinsic[:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mrotation_matrix \u001b[38;5;241m@\u001b[39m eye\n\u001b[1;32m     18\u001b[0m view_control \u001b[38;5;241m=\u001b[39m visualizer\u001b[38;5;241m.\u001b[39mget_view_control()\n\u001b[0;32m---> 19\u001b[0m camera_params \u001b[38;5;241m=\u001b[39m \u001b[43mview_control\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_pinhole_camera_parameters\u001b[49m()\n\u001b[1;32m     20\u001b[0m camera_params\u001b[38;5;241m.\u001b[39mextrinsic \u001b[38;5;241m=\u001b[39m extrinsic\n\u001b[1;32m     21\u001b[0m view_control\u001b[38;5;241m.\u001b[39mconvert_from_pinhole_camera_parameters(camera_params)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'convert_to_pinhole_camera_parameters'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 300x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mesh = open3d.io.read_triangle_mesh('/home/user2/wns/cad-recode/tmp/1.stl')\n",
    "mesh.vertices = open3d.utility.Vector3dVector(np.asarray(mesh.vertices) / 100.)\n",
    "mesh.paint_uniform_color(np.array([255, 255, 136]) / 255)\n",
    "mesh.compute_vertex_normals()\n",
    "plt.figure(figsize=(3, 3))\n",
    "_ = plt.imshow(render(mesh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute IoU and Chamfer distance metrics\n",
    "The predicted model resembles a table, but let's calculate how closely it matches the ground truth mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CD: 0.283, IoU: 0.943\n"
     ]
    }
   ],
   "source": [
    "pred_mesh = trimesh.load_mesh('/home/user2/wns/cad-recode/tmp/1.stl')\n",
    "pred_mesh.apply_transform(trimesh.transformations.scale_matrix(1 / 100 / 2))\n",
    "pred_mesh.apply_transform(trimesh.transformations.translation_matrix([0.5, 0.5, 0.5]))\n",
    "gt_mesh.apply_transform(trimesh.transformations.scale_matrix(1 / 2))\n",
    "gt_mesh.apply_transform(trimesh.transformations.translation_matrix([0.5, 0.5, 0.5]))\n",
    "\n",
    "n_points = 8192\n",
    "gt_points, _ = trimesh.sample.sample_surface(gt_mesh, n_points)\n",
    "pred_points, _ = trimesh.sample.sample_surface(pred_mesh, n_points)\n",
    "gt_distance, _ = cKDTree(gt_points).query(pred_points, k=1)\n",
    "pred_distance, _ = cKDTree(pred_points).query(gt_points, k=1)\n",
    "cd = np.mean(np.square(gt_distance)) + np.mean(np.square(pred_distance))\n",
    "\n",
    "intersection_volume = 0\n",
    "for gt_mesh_i in gt_mesh.split():\n",
    "    for pred_mesh_i in pred_mesh.split():\n",
    "        intersection = gt_mesh_i.intersection(pred_mesh_i)\n",
    "        volume = intersection.volume if intersection is not None else 0\n",
    "        intersection_volume += volume\n",
    "\n",
    "gt_volume = sum(m.volume for m in gt_mesh.split())\n",
    "pred_volume = sum(m.volume for m in pred_mesh.split())\n",
    "union_volume = gt_volume + pred_volume - intersection_volume\n",
    "iou = intersection_volume / union_volume\n",
    "\n",
    "print(f'CD: {cd * 1000:.3f}, IoU: {iou:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cadrecode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
