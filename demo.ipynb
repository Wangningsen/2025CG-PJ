{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAD-Recode Demonstration\n",
    "In this demonstration, we load a point cloud and process it using a pre-trained CAD-Recode model, which generates Python CadQuery code. Finally, we interpret the code to create a CAD model, visualize it, and evaluate its quality metrics. This notebook works on both CPU and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CAD-Recode model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d\n",
    "import trimesh\n",
    "import skimage.io\n",
    "import numpy as np\n",
    "import cadquery as cq\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoTokenizer, Qwen2ForCausalLM, Qwen2Model, PreTrainedModel)\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from pytorch3d.ops import sample_farthest_points\n",
    "\n",
    "\n",
    "class FourierPointEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        frequencies = 2.0 ** torch.arange(8, dtype=torch.float32)\n",
    "        self.register_buffer('frequencies', frequencies, persistent=False)\n",
    "        self.projection = nn.Linear(51, hidden_size)\n",
    "\n",
    "    def forward(self, points):\n",
    "        x = points\n",
    "        x = (x.unsqueeze(-1) * self.frequencies).view(*x.shape[:-1], -1)\n",
    "        x = torch.cat((points, x.sin(), x.cos()), dim=-1)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CADRecode(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        PreTrainedModel.__init__(self, config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        \n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        self.point_encoder = FourierPointEncoder(config.hidden_size)\n",
    "        torch.set_default_dtype(torch.bfloat16)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                point_cloud=None,\n",
    "                position_ids=None,\n",
    "                past_key_values=None,\n",
    "                inputs_embeds=None,\n",
    "                labels=None,\n",
    "                use_cache=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None,\n",
    "                cache_position=None):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # concatenate point and text embeddings\n",
    "        if past_key_values is None or past_key_values.get_seq_length() == 0:\n",
    "            assert inputs_embeds is None\n",
    "            inputs_embeds = self.model.embed_tokens(input_ids)\n",
    "            point_embeds = self.point_encoder(point_cloud).bfloat16()\n",
    "            inputs_embeds[attention_mask == -1] = point_embeds.reshape(-1, point_embeds.shape[2])\n",
    "            attention_mask[attention_mask == -1] = 1\n",
    "            input_ids = None\n",
    "            position_ids = None\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position)\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions)\n",
    "\n",
    "    def prepare_inputs_for_generation(self, *args, **kwargs):\n",
    "        model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n",
    "        model_inputs['point_cloud'] = kwargs['point_cloud']\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CAD-Recode checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "attn_implementation = 'flash_attention_2' if torch.cuda.is_available() else None\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    '/home/user2/wns/cad-recode/Qwen2-1.5B',\n",
    "    pad_token='<|im_end|>',\n",
    "    padding_side='left')\n",
    "model = CADRecode.from_pretrained(\n",
    "    '/home/user2/wns/cad-recode/cad-recode-v1.5',\n",
    "    torch_dtype='auto',\n",
    "    attn_implementation=attn_implementation).eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load input point cloud\n",
    "Load the mesh of one of the examples from the DeepCAD test set and sample 256 points from it. Then input mesh is normalized to fit within a cube of size 2, centered at the origin. Both the mesh and point cloud can be visualized interactively in this notebook using `open3d` or `trimesh`; however, for non-interactive environments, we render them as static images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/filaPro/cad-recode/releases/download/v1.0/00409751.stl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mesh_to_point_cloud(mesh, n_points=256, n_pre_points=8192):\n",
    "    vertices, _ = trimesh.sample.sample_surface(mesh, n_pre_points)\n",
    "    _, ids = sample_farthest_points(torch.tensor(vertices).unsqueeze(0), K=n_points)\n",
    "    ids = ids[0].numpy()\n",
    "    return np.asarray(vertices[ids])\n",
    "\n",
    "\n",
    "gt_mesh = trimesh.load_mesh('./00409751.stl')\n",
    "gt_mesh.apply_translation(-(gt_mesh.bounds[0] + gt_mesh.bounds[1]) / 2.0)\n",
    "gt_mesh.apply_scale(2.0 / max(gt_mesh.extents))\n",
    "np.random.seed(0)\n",
    "point_cloud = mesh_to_point_cloud(gt_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look at input point cloud\n",
    "It looks like a table with four legs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] Failed to initialize GLEW.\u001b[0;m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'convert_to_pinhole_camera_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m pcd\u001b[38;5;241m.\u001b[39mpaint_uniform_color(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m239\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m     40\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m---> 41\u001b[0m _ \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcd\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[23], line 19\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(geometry, image_size, camera_distance, image_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m extrinsic[:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mrotation_matrix \u001b[38;5;241m@\u001b[39m eye\n\u001b[1;32m     18\u001b[0m view_control \u001b[38;5;241m=\u001b[39m visualizer\u001b[38;5;241m.\u001b[39mget_view_control()\n\u001b[0;32m---> 19\u001b[0m camera_params \u001b[38;5;241m=\u001b[39m \u001b[43mview_control\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_pinhole_camera_parameters\u001b[49m()\n\u001b[1;32m     20\u001b[0m camera_params\u001b[38;5;241m.\u001b[39mextrinsic \u001b[38;5;241m=\u001b[39m extrinsic\n\u001b[1;32m     21\u001b[0m view_control\u001b[38;5;241m.\u001b[39mconvert_from_pinhole_camera_parameters(camera_params)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'convert_to_pinhole_camera_parameters'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 300x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def render(geometry, image_size=128, camera_distance=-1.8, image_path='/home/user2/wns/cad-recode/tmp/tmp.png'):\n",
    "    visualizer = open3d.visualization.Visualizer()\n",
    "    visualizer.create_window(visible=False)\n",
    "    visualizer.add_geometry(geometry)\n",
    "\n",
    "    lookat = np.array([0, 0, 0], dtype=np.float32)\n",
    "    front = np.array([1, 1, 1], dtype=np.float32)\n",
    "    up = np.array([0, 1, 0], dtype=np.float32)\n",
    "    eye = lookat + front * camera_distance\n",
    "    right = np.cross(up, front)\n",
    "    right /= np.linalg.norm(right)\n",
    "    true_up = np.cross(front, right)\n",
    "    rotation_matrix = np.column_stack((right, true_up, front)).T\n",
    "    extrinsic = np.eye(4)\n",
    "    extrinsic[:3, :3] = rotation_matrix\n",
    "    extrinsic[:3, 3] = -rotation_matrix @ eye\n",
    "\n",
    "    view_control = visualizer.get_view_control()\n",
    "    camera_params = view_control.convert_to_pinhole_camera_parameters()\n",
    "    camera_params.extrinsic = extrinsic\n",
    "    view_control.convert_from_pinhole_camera_parameters(camera_params)\n",
    "\n",
    "    visualizer.poll_events()\n",
    "    visualizer.update_renderer()\n",
    "    visualizer.capture_screen_image(image_path)\n",
    "    visualizer.destroy_window()\n",
    "\n",
    "    image = skimage.io.imread(image_path)\n",
    "    image = skimage.transform.resize(\n",
    "        image,\n",
    "        output_shape=(image_size, image_size),\n",
    "        order=2,\n",
    "        anti_aliasing=True,\n",
    "        preserve_range=True).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "pcd = open3d.geometry.PointCloud()\n",
    "pcd.points = open3d.utility.Vector3dVector(point_cloud)\n",
    "pcd.paint_uniform_color(np.array([0, 80, 239]) / 255)\n",
    "plt.figure(figsize=(3, 3))\n",
    "_ = plt.imshow(render(pcd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CAD-Recode on the input point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import cadquery as cq\n",
      "w0=cq.Workplane('XY',origin=(0,0,88))\n",
      "r=w0.sketch().segment((-98,-100),(-77,-99)).arc((-78,-89),(-77,-79)).segment((-98,-79)).close().assemble().reset().face(w0.sketch().segment((-98,78),(-76,78)).segment((-75,100)).segment((-98,100)).close().assemble()).reset().face(w0.sketch().segment((78,-99),(98,-99)).segment((98,-79)).segment((78,-79)).arc((79,-89),(78,-99)).assemble()).reset().face(w0.sketch().arc((78,82),(88,82),(98,80)).segment((98,100)).segment((78,100)).close().assemble()).finalize().extrude(-177).union(w0.workplane(offset=-15/2).box(196,200,15))\n"
     ]
    }
   ],
   "source": [
    "input_ids = [tokenizer.pad_token_id] * len(point_cloud) + [tokenizer('<|im_start|>')['input_ids'][0]]\n",
    "attention_mask = [-1] * len(point_cloud) + [1]\n",
    "with torch.no_grad():\n",
    "    batch_ids = model.generate(\n",
    "        input_ids=torch.tensor(input_ids).unsqueeze(0).to(model.device),\n",
    "        attention_mask=torch.tensor(attention_mask).unsqueeze(0).to(model.device),\n",
    "        point_cloud=torch.tensor(point_cloud.astype(np.float32)).unsqueeze(0).to(model.device),\n",
    "        max_new_tokens=768,\n",
    "        pad_token_id=tokenizer.pad_token_id)\n",
    "py_string = tokenizer.batch_decode(batch_ids)[0]\n",
    "begin = py_string.find('<|im_start|>') + 12\n",
    "end = py_string.find('<|endoftext|>')\n",
    "py_string = py_string[begin: end]\n",
    "print(py_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute predicted python code to raise a CAD model\n",
    "The predicted code might be invalid or could potentially cause memory leaks in CadQuery, as described in this [issue](https://github.com/CadQuery/cadquery/issues/1665).  Therefore, it is recommended to run the next cell in a separate process with a timeout of, for example, 3 seconds.:\n",
    "```\n",
    "process = Process(target=..., args=...)\n",
    "process.start()\n",
    "process.join(3)\n",
    "\n",
    "if process.is_alive():\n",
    "    process.terminate()\n",
    "    process.join()\n",
    "```\n",
    "But in this demo safety is omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(py_string, globals())\n",
    "compound = globals()['r'].val()\n",
    "vertices, faces = compound.tessellate(0.001, 0.1)\n",
    "mesh = trimesh.Trimesh([(v.x, v.y, v.z) for v in vertices], faces)\n",
    "mesh.export('/home/user2/wns/cad-recode/tmp/1.stl')\n",
    "cq.exporters.export(compound, '/home/user2/wns/cad-recode/tmp/1.step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] Failed to initialize GLEW.\u001b[0;m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'convert_to_pinhole_camera_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m mesh\u001b[38;5;241m.\u001b[39mcompute_vertex_normals()\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m _ \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[23], line 19\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(geometry, image_size, camera_distance, image_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m extrinsic[:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mrotation_matrix \u001b[38;5;241m@\u001b[39m eye\n\u001b[1;32m     18\u001b[0m view_control \u001b[38;5;241m=\u001b[39m visualizer\u001b[38;5;241m.\u001b[39mget_view_control()\n\u001b[0;32m---> 19\u001b[0m camera_params \u001b[38;5;241m=\u001b[39m \u001b[43mview_control\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_pinhole_camera_parameters\u001b[49m()\n\u001b[1;32m     20\u001b[0m camera_params\u001b[38;5;241m.\u001b[39mextrinsic \u001b[38;5;241m=\u001b[39m extrinsic\n\u001b[1;32m     21\u001b[0m view_control\u001b[38;5;241m.\u001b[39mconvert_from_pinhole_camera_parameters(camera_params)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'convert_to_pinhole_camera_parameters'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 300x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mesh = open3d.io.read_triangle_mesh('/home/user2/wns/cad-recode/tmp/1.stl')\n",
    "mesh.vertices = open3d.utility.Vector3dVector(np.asarray(mesh.vertices) / 100.)\n",
    "mesh.paint_uniform_color(np.array([255, 255, 136]) / 255)\n",
    "mesh.compute_vertex_normals()\n",
    "plt.figure(figsize=(3, 3))\n",
    "_ = plt.imshow(render(mesh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute IoU and Chamfer distance metrics\n",
    "The predicted model resembles a table, but let's calculate how closely it matches the ground truth mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CD: 0.283, IoU: 0.943\n"
     ]
    }
   ],
   "source": [
    "pred_mesh = trimesh.load_mesh('/home/user2/wns/cad-recode/tmp/1.stl')\n",
    "pred_mesh.apply_transform(trimesh.transformations.scale_matrix(1 / 100 / 2))\n",
    "pred_mesh.apply_transform(trimesh.transformations.translation_matrix([0.5, 0.5, 0.5]))\n",
    "gt_mesh.apply_transform(trimesh.transformations.scale_matrix(1 / 2))\n",
    "gt_mesh.apply_transform(trimesh.transformations.translation_matrix([0.5, 0.5, 0.5]))\n",
    "\n",
    "n_points = 8192\n",
    "gt_points, _ = trimesh.sample.sample_surface(gt_mesh, n_points)\n",
    "pred_points, _ = trimesh.sample.sample_surface(pred_mesh, n_points)\n",
    "gt_distance, _ = cKDTree(gt_points).query(pred_points, k=1)\n",
    "pred_distance, _ = cKDTree(pred_points).query(gt_points, k=1)\n",
    "cd = np.mean(np.square(gt_distance)) + np.mean(np.square(pred_distance))\n",
    "\n",
    "intersection_volume = 0\n",
    "for gt_mesh_i in gt_mesh.split():\n",
    "    for pred_mesh_i in pred_mesh.split():\n",
    "        intersection = gt_mesh_i.intersection(pred_mesh_i)\n",
    "        volume = intersection.volume if intersection is not None else 0\n",
    "        intersection_volume += volume\n",
    "\n",
    "gt_volume = sum(m.volume for m in gt_mesh.split())\n",
    "pred_volume = sum(m.volume for m in pred_mesh.split())\n",
    "union_volume = gt_volume + pred_volume - intersection_volume\n",
    "iou = intersection_volume / union_volume\n",
    "\n",
    "print(f'CD: {cd * 1000:.3f}, IoU: {iou:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cadrecode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
